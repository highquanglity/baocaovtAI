% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{vaswani2023attentionneed}
\BIBentryALTinterwordspacing
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/1706.03762}
\BIBentrySTDinterwordspacing

\bibitem{vae}
\BIBentryALTinterwordspacing
D.~P. Kingma and M.~Welling, ``An introduction to variational autoencoders,''
  \emph{Foundations and Trends® in Machine Learning}, vol.~12, no.~4, p.
  307–392, 2019. [Online]. Available:
  \url{http://dx.doi.org/10.1561/2200000056}
\BIBentrySTDinterwordspacing

\bibitem{gans}
\BIBentryALTinterwordspacing
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' 2014.
  [Online]. Available: \url{https://arxiv.org/abs/1406.2661}
\BIBentrySTDinterwordspacing

\bibitem{stablediffusion}
\BIBentryALTinterwordspacing
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2112.10752}
\BIBentrySTDinterwordspacing

\bibitem{openai2023dalle3}
OpenAI, ``Improving image generation with better captions,''
  \url{https://cdn.openai.com/papers/dall-e-3.pdf}, 2023, accessed: 2025-05-15.

\bibitem{openai2023gpt4}
OpenAI and Others, ``Gpt-4 technical report,''
  \url{https://arxiv.org/abs/2303.08774}, 2023, accessed: 2025-05-15.

\bibitem{google2023gemini}
G.~DeepMind, ``Gemini: A family of highly capable multimodal models,''
  \url{https://arxiv.org/abs/2312.11805}, 2023, accessed: 2025-05-15.

\bibitem{clip}
\BIBentryALTinterwordspacing
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever, ``Learning
  transferable visual models from natural language supervision,'' in
  \emph{Proceedings of the 38th International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 8748--8763.
  [Online]. Available: \url{https://proceedings.mlr.press/v139/radford21a.html}
\BIBentrySTDinterwordspacing

\bibitem{alayrac2022flamingo}
\BIBentryALTinterwordspacing
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, F.~Momeni,
  S.~Milani, P.-Y. Huang, I.~Laptev \emph{et~al.}, ``Flamingo: a visual
  language model for few-shot learning,'' \emph{arXiv preprint
  arXiv:2204.14198}, 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2204.14198}
\BIBentrySTDinterwordspacing

\bibitem{touvron2023llama}
\BIBentryALTinterwordspacing
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozière, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin,
  E.~Grave, and G.~Lample, ``Llama: Open and efficient foundation language
  models,'' \emph{arXiv preprint arXiv:2302.13971}, 2023. [Online]. Available:
  \url{https://arxiv.org/abs/2302.13971}
\BIBentrySTDinterwordspacing

\bibitem{yang2024qwen2}
\BIBentryALTinterwordspacing
B.~Yang \emph{et~al.}, ``Qwen2 technical report,'' \emph{arXiv preprint
  arXiv:2407.10671}, 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2407.10671}
\BIBentrySTDinterwordspacing

\bibitem{chen2024internvl}
Z.~Chen, J.~Wu, W.~Wang, W.~Su, G.~Chen, S.~Xing, M.~Zhong, Q.~Zhang, X.~Zhu,
  L.~Lu \emph{et~al.}, ``Internvl: Scaling up vision foundation models and
  aligning for generic visual-linguistic tasks,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024, pp.
  24\,185--24\,198.

\bibitem{nguyen2020phobertpretrainedlanguagemodels}
\BIBentryALTinterwordspacing
D.~Q. Nguyen and A.~T. Nguyen, ``Phobert: Pre-trained language models for
  vietnamese,'' 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2003.00744}
\BIBentrySTDinterwordspacing

\bibitem{doan2024vintern1befficientmultimodallarge}
\BIBentryALTinterwordspacing
K.~T. Doan, B.~G. Huynh, D.~T. Hoang, T.~D. Pham, N.~H. Pham, Q.~T.~M. Nguyen,
  B.~Q. Vo, and S.~N. Hoang, ``Vintern-1b: An efficient multimodal large
  language model for vietnamese,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2408.12480}
\BIBentrySTDinterwordspacing

\bibitem{nilsback2008automated}
\BIBentryALTinterwordspacing
M.-E. Nilsback and A.~Zisserman, ``Automated flower classification over a large
  number of classes,'' \emph{Indian Conference on Computer Vision, Graphics and
  Image Processing}, 2008. [Online]. Available:
  \url{https://www.robots.ox.ac.uk/~vgg/data/flowers/102/}
\BIBentrySTDinterwordspacing

\bibitem{rashtchian2010collecting}
\BIBentryALTinterwordspacing
C.~Rashtchian, P.~Young, M.~Hodosh, and J.~Hockenmaier, ``Collecting image
  annotations using amazon’s mechanical turk,'' in \emph{Proceedings of the
  NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s
  Mechanical Turk}, 2010, pp. 139--147. [Online]. Available:
  \url{https://hockenmaier.cs.illinois.edu/pubs/rashtchian-et-al10.pdf}
\BIBentrySTDinterwordspacing

\bibitem{ordonez2011im2text}
\BIBentryALTinterwordspacing
V.~Ordonez, G.~Kulkarni, and T.~L. Berg, ``Im2text: Describing images using 1
  million captioned photographs,'' in \emph{Advances in neural information
  processing systems}, 2011, pp. 1143--1151. [Online]. Available:
  \url{https://www.cs.unc.edu/~vso/publication/im2text.html}
\BIBentrySTDinterwordspacing

\bibitem{plummer2016flickr30kentitiescollectingregiontophrase}
\BIBentryALTinterwordspacing
B.~A. Plummer, L.~Wang, C.~M. Cervantes, J.~C. Caicedo, J.~Hockenmaier, and
  S.~Lazebnik, ``Flickr30k entities: Collecting region-to-phrase
  correspondences for richer image-to-sentence models,'' 2016. [Online].
  Available: \url{https://arxiv.org/abs/1505.04870}
\BIBentrySTDinterwordspacing

\bibitem{lin2014microsoft}
\BIBentryALTinterwordspacing
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' \emph{European conference on computer vision}, pp. 740--755, 2014.
  [Online]. Available: \url{https://cocodataset.org}
\BIBentrySTDinterwordspacing

\bibitem{vedantam2015cider}
R.~Vedantam, C.~Lawrence~Zitnick, and D.~Parikh, ``Cider: Consensus-based image
  description evaluation,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2015, pp. 4566--4575.

\bibitem{chen2015microsoftcococaptionsdata}
\BIBentryALTinterwordspacing
X.~Chen, H.~Fang, T.-Y. Lin, R.~Vedantam, S.~Gupta, P.~Dollar, and C.~L.
  Zitnick, ``Microsoft coco captions: Data collection and evaluation server,''
  2015. [Online]. Available: \url{https://arxiv.org/abs/1504.00325}
\BIBentrySTDinterwordspacing

\bibitem{antol2015vqa}
\BIBentryALTinterwordspacing
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and
  D.~Parikh, ``Vqa: Visual question answering,'' in \emph{Proceedings of the
  IEEE international conference on computer vision}, 2015, pp. 2425--2433.
  [Online]. Available: \url{https://visualqa.org}
\BIBentrySTDinterwordspacing

\bibitem{kazemi2017show}
\BIBentryALTinterwordspacing
V.~Kazemi and A.~Elqursh, ``Show, ask, attend, and answer: A strong baseline
  for visual question answering,'' \emph{arXiv preprint arXiv:1611.08321},
  2017. [Online]. Available: \url{https://arxiv.org/abs/1611.08321}
\BIBentrySTDinterwordspacing

\bibitem{krishna2017visual}
\BIBentryALTinterwordspacing
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A.~E. Shamma, M.~S. Bernstein, and L.~Fei-Fei,
  ``Visual genome: Connecting language and vision using crowdsourced dense
  image annotations,'' in \emph{International journal of computer vision}, vol.
  123, no.~1, 2017, pp. 32--73. [Online]. Available:
  \url{https://visualgenome.org}
\BIBentrySTDinterwordspacing

\bibitem{goyal2017making}
\BIBentryALTinterwordspacing
Y.~Goyal, T.~Khot, D.~Summers-Stay, D.~Batra, and D.~Parikh, ``Making the v in
  vqa matter: Elevating the role of image understanding in visual question
  answering,'' \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp. 6904--6913, 2017. [Online]. Available:
  \url{https://visualqa.org}
\BIBentrySTDinterwordspacing

\bibitem{zhang2019showattendtranslateunpaired}
\BIBentryALTinterwordspacing
H.~Zhang, W.~Chen, J.~Tian, Y.~Wang, and Y.~Jin, ``Show, attend and translate:
  Unpaired multi-domain image-to-image translation with visual attention,''
  2019. [Online]. Available: \url{https://arxiv.org/abs/1811.07483}
\BIBentrySTDinterwordspacing

\bibitem{agrawal2019nocaps}
H.~Agrawal, K.~Desai, Y.~Wang, G.~Chechik, A.~Berg, D.~Parikh, and D.~Batra,
  ``nocaps: novel object captioning at scale,'' in \emph{Proceedings of the
  IEEE/CVF International Conference on Computer Vision}, 2019, pp. 8948--8957.

\bibitem{zellers2019recognition}
R.~Zellers, Y.~Bisk, A.~Farhadi, and Y.~Choi, ``From recognition to cognition:
  Visual commonsense reasoning,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2019, pp. 6720--6731.

\bibitem{sharma2018conceptual}
\BIBentryALTinterwordspacing
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut, ``Conceptual captions: A
  cleaned, hypernymed, image alt-text dataset for automatic image captioning,''
  2018. [Online]. Available: \url{https://arxiv.org/abs/1803.09123}
\BIBentrySTDinterwordspacing

\bibitem{schuhmann2021laion400m}
\BIBentryALTinterwordspacing
C.~Schuhmann, R.~Beaumont, R.~Vencu, R.~Wightman, M.~Cherti, T.~Coombes,
  R.~Rombach, P.~Jenicek, P.~Wilke, C.~Schulz \emph{et~al.}, ``Laion-400m: Open
  dataset of clip-filtered 400 million image-text pairs,'' 2021. [Online].
  Available: \url{https://arxiv.org/abs/2111.02114}
\BIBentrySTDinterwordspacing

\bibitem{feng2024cc500}
W.~Feng, X.~He, T.-J. Fu, V.~Jampani, A.~R. Akula, P.~Narayana, S.~Basu, X.~E.
  Wang, and W.~Y. Wang, ``Concept conjunction 500 (cc-500),''
  \url{https://conceptconjunction.github.io/}, 2024, dataset published by the
  authors.

\bibitem{changpinyo2021cc12m}
\BIBentryALTinterwordspacing
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut, ``Conceptual 12m: Pushing
  web-scale image-text pretraining to recognize long-tail visual concepts,''
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp. 3558--3568, 2021. [Online]. Available:
  \url{https://github.com/google-research-datasets/conceptual-12m}
\BIBentrySTDinterwordspacing

\bibitem{wang2022diffusiondb}
\BIBentryALTinterwordspacing
Z.~J. Wang, E.~Montoya, D.~Munechika, H.~Yang, B.~Hoover, and D.~H. Chau,
  ``Diffusiondb: A large-scale prompt gallery dataset for text-to-image
  generative models,'' \emph{arXiv preprint arXiv:2210.14896}, 2022. [Online].
  Available: \url{https://arxiv.org/abs/2210.14896}
\BIBentrySTDinterwordspacing

\bibitem{bitton2022winoground}
\BIBentryALTinterwordspacing
J.~Bitton, A.~Diwan, N.~Goyal, A.~Kembhavi, A.~Farhadi, D.~Parikh, and
  D.~Batra, ``Winoground: Probing vision and language models for
  visio-linguistic compositionality,'' \emph{arXiv preprint arXiv:2204.03162},
  2022. [Online]. Available: \url{https://arxiv.org/abs/2204.03162}
\BIBentrySTDinterwordspacing

\bibitem{saharia2022photorealistic}
\BIBentryALTinterwordspacing
C.~Saharia, W.~Chang, J.~Ho, T.~Salimans, J.~Ho, T.~Salimans, J.~Ho,
  T.~Salimans, J.~Ho, and T.~Salimans, ``Photorealistic text-to-image diffusion
  models with deep language understanding,'' \emph{arXiv preprint
  arXiv:2205.11487}, 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2205.11487}
\BIBentrySTDinterwordspacing

\bibitem{zhang2024abc6k}
Y.~Zhang, P.~Yu, and Y.~N. Wu, ``Abc-6k dataset,''
  \url{https://doi.org/10.57702/nc9rsac4}, 2024, dataset released by the
  authors.

\bibitem{schramowski2023safelatentdiffusionmitigating}
\BIBentryALTinterwordspacing
P.~Schramowski, M.~Brack, B.~Deiseroth, and K.~Kersting, ``Safe latent
  diffusion: Mitigating inappropriate degeneration in diffusion models,'' 2023.
  [Online]. Available: \url{https://arxiv.org/abs/2211.05105}
\BIBentrySTDinterwordspacing

\bibitem{huang2023t2icompbench}
\BIBentryALTinterwordspacing
K.~Huang, K.~Sun, E.~Xie, Z.~Li, and X.~Liu, ``T2i-compbench: A comprehensive
  benchmark for open-world compositional text-to-image generation,''
  \emph{Advances in Neural Information Processing Systems}, vol.~36, pp.
  78\,723--78\,747, 2023. [Online]. Available:
  \url{https://proceedings.neurips.cc/paper_files/paper/2023/hash/f8ad010cdd9143dbb0e9308c093aff24-Abstract-Datasets_and_Benchmarks.html}
\BIBentrySTDinterwordspacing

\bibitem{cho2023dallevalprobingreasoningskills}
\BIBentryALTinterwordspacing
J.~Cho, A.~Zala, and M.~Bansal, ``Dall-eval: Probing the reasoning skills and
  social biases of text-to-image generation models,'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2202.04053}
\BIBentrySTDinterwordspacing

\bibitem{liang2023richhf}
\BIBentryALTinterwordspacing
Y.~Liang, J.~He, G.~Li, P.~Li, A.~Klimovskiy, N.~Carolan, J.~Sun,
  J.~Pont-Tuset, S.~Young, F.~Yang, J.~Ke, K.~Dvijotham, K.~Collins, Y.~Luo,
  Y.~Li, K.~J. Kohlhoff, D.~Ramachandran, and V.~Navalpakkam, ``Rich human
  feedback for text-to-image generation,'' in \emph{Proceedings of the 2023
  Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  2023. [Online]. Available: \url{https://arxiv.org/abs/2312.10240}
\BIBentrySTDinterwordspacing

\bibitem{chen2023textdiffuser}
\BIBentryALTinterwordspacing
J.~Chen, Y.~Huang, T.~Lv, and F.~Wei, ``Textdiffuser: Diffusion models as text
  painters,'' \emph{arXiv preprint arXiv:2305.10855}, 2023. [Online].
  Available: \url{https://arxiv.org/abs/2305.10855}
\BIBentrySTDinterwordspacing

\bibitem{changpinyo2021conceptual12m}
\BIBentryALTinterwordspacing
S.~Changpinyo, P.~Sharma, N.~Ding, and S.~Goodman, ``Conceptual 12m: Pushing
  web-scale image-text pre-training to recognize long-tail visual concepts,''
  \emph{arXiv preprint arXiv:2102.08981}, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2102.08981}
\BIBentrySTDinterwordspacing

\bibitem{pham2024ktvic}
\BIBentryALTinterwordspacing
A.-C. Pham, V.-Q. Nguyen, T.-H. Vuong, and Q.-T. Ha, ``Ktvic: A vietnamese
  image captioning dataset on the life domain,'' \emph{arXiv preprint
  arXiv:2401.08100}, 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2401.08100}
\BIBentrySTDinterwordspacing

\bibitem{lam2020uitviic}
\BIBentryALTinterwordspacing
Q.~H. Lam, Q.~D. Le, K.~V. Nguyen, and N.~L.-T. Nguyen, ``Uit-viic: A dataset
  for the first evaluation on vietnamese image captioning,'' \emph{arXiv
  preprint arXiv:2002.00175}, 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2002.00175}
\BIBentrySTDinterwordspacing

\bibitem{bui2023uitopenviic}
\BIBentryALTinterwordspacing
D.~C. Bui, N.~H. Nguyen, and K.~Nguyen, ``Uit-openviic: A novel benchmark for
  evaluating image captioning in vietnamese,'' \emph{arXiv preprint
  arXiv:2305.04166}, 2023. [Online]. Available:
  \url{https://arxiv.org/abs/2305.04166}
\BIBentrySTDinterwordspacing

\bibitem{dinhanhx_VisualRoBERTa_2022}
\BIBentryALTinterwordspacing
dinhanhx, ``{VisualRoBERTa},'' 9 2022. [Online]. Available:
  \url{https://github.com/dinhanhx/VisualRoBERTa}
\BIBentrySTDinterwordspacing

\bibitem{pham2024viocrvqa}
\BIBentryALTinterwordspacing
H.~Q. Pham, T.~K.-B. Nguyen, Q.~V. Nguyen, D.~Q. Tran, N.~H. Nguyen, K.~V.
  Nguyen, and N.~L.-T. Nguyen, ``Viocrvqa: Novel benchmark dataset and vision
  reader for visual question answering by understanding vietnamese text in
  images,'' \emph{arXiv preprint arXiv:2404.18397}, 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2404.18397}
\BIBentrySTDinterwordspacing

\bibitem{nexdata2024vietnameseocr}
\BIBentryALTinterwordspacing
Nexdata-AI, ``4,995 vietnamese ocr images dataset,''
  \url{https://www.nexdata.ai/datasets/ocr/1059?source=Github}, Nexdata-AI,
  China, 2024, accessed: 2025-05-15. [Online]. Available:
  \url{https://www.nexdata.ai/datasets/ocr/1059?source=Github}
\BIBentrySTDinterwordspacing

\bibitem{li2023blip2}
J.~Li, D.~Yang, P.~Su, C.~Lu, X.~Li, and Q.~V. Le, ``Blip-2: Bootstrapped
  language-image pre-training with frozen image encoders and large language
  models,'' \emph{arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{vistral2024vietnamese}
V.~A. Team, ``Vistral-v-7b: Vietnamese vision-language model,'' 2024,
  \url{https://github.com/Vistral/vistral-v-7b}.

\bibitem{paddleocr2021}
{PaddlePaddle Authors}, ``Paddleocr: An open-source ocr system based on
  paddlepaddle,'' \url{https://github.com/PaddlePaddle/PaddleOCR}, 2021,
  accessed: 2025-05-15.

\bibitem{vietocr2020}
H.~Nguyen, ``Vietocr: Open-source optical character recognition for
  vietnamese,'' \url{https://github.com/duyquang/vietocr}, 2020, accessed:
  2025-05-15.

\bibitem{stablediffusionxl2023}
S.~AI and Collaborators, ``Stable diffusion xl: Scaling latent diffusion models
  to 1 billion parameters,''
  \url{https://stability.ai/blog/stable-diffusion-xl-release}, 2023, accessed:
  2025-05-15.

\bibitem{stablediffusion35}
S.~AI, ``Stable diffusion 3.5,''
  \url{https://stability.ai/blog/stable-diffusion-3-5-release}, 2024, accessed:
  2025-05-15.

\bibitem{controlnet2023}
L.~Zhang, M.~A. Yang, and A.~A. Efros, ``Controlnet: Adding conditional control
  to text-to-image diffusion models,'' 2023,
  \url{https://arxiv.org/abs/2302.05543}.

\bibitem{yang2024158bitflux}
\BIBentryALTinterwordspacing
C.~Yang, C.~Liu, X.~Deng, D.~Kim, X.~Mei, X.~Shen, and L.-C. Chen, ``1.58-bit
  flux,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2412.18653}
\BIBentrySTDinterwordspacing

\bibitem{hamdi2025vistaocrgenerativeinteractiveend}
\BIBentryALTinterwordspacing
L.~Hamdi, A.~Tamasna, P.~Boisson, and T.~Paquet, ``Vista-ocr: Towards
  generative and interactive end to end ocr models,'' 2025. [Online].
  Available: \url{https://arxiv.org/abs/2504.03621}
\BIBentrySTDinterwordspacing

\bibitem{xue2021mt5massivelymultilingualpretrained}
\BIBentryALTinterwordspacing
L.~Xue, N.~Constant, A.~Roberts, M.~Kale, R.~Al-Rfou, A.~Siddhant, A.~Barua,
  and C.~Raffel, ``mt5: A massively multilingual pre-trained text-to-text
  transformer,'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2010.11934}
\BIBentrySTDinterwordspacing

\end{thebibliography}
