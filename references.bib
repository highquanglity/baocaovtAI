@article{vae,
	title={An Introduction to Variational Autoencoders},
	volume={12},
	ISSN={1935-8245},
	url={http://dx.doi.org/10.1561/2200000056},
	DOI={10.1561/2200000056},
	number={4},
	journal={Foundations and Trends® in Machine Learning},
	publisher={Now Publishers},
	author={Kingma, Diederik P. and Welling, Max},
	year={2019},
	pages={307–392} }

@misc{gans,
	title={Generative Adversarial Networks}, 
	author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
	year={2014},
	eprint={1406.2661},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1406.2661}, 
}

@misc{stablediffusion,
	title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
	author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
	year={2022},
	eprint={2112.10752},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2112.10752}, 
}

@misc{openai2023dalle3,
	title        = {Improving Image Generation with Better Captions},
	author       = {OpenAI},
	year         = {2023},
	howpublished = {\url{https://cdn.openai.com/papers/dall-e-3.pdf}},
	note         = {Accessed: 2025-05-15}
}

@misc{openai2023gpt4,
	title        = {GPT-4 Technical Report},
	author       = {OpenAI and Others},
	year         = {2023},
	howpublished = {\url{https://arxiv.org/abs/2303.08774}},
	note         = {Accessed: 2025-05-15}
}

@misc{google2023gemini,
	author       = {Google DeepMind},
	title        = {Gemini: A Family of Highly Capable Multimodal Models},
	year         = {2023},
	howpublished = {\url{https://arxiv.org/abs/2312.11805}},
	note         = {Accessed: 2025-05-15}
}

@inproceedings{clip,
	title={Learning Transferable Visual Models From Natural Language Supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	booktitle={Proceedings of the 38th International Conference on Machine Learning},
	pages={8748--8763},
	year={2021},
	organization={PMLR},
	url={https://proceedings.mlr.press/v139/radford21a.html}
}

@article{alayrac2022flamingo,
	title={Flamingo: a Visual Language Model for Few-Shot Learning},
	author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Paul and Miech, Antoine and Barr, Ian and Hasson, Yana and Momeni, Fadime and Milani, Sander and Huang, Po-Yao and Laptev, Ivan and others},
	journal={arXiv preprint arXiv:2204.14198},
	year={2022},
	url={https://arxiv.org/abs/2204.14198}
}

@article{touvron2023llama,
	title={LLaMA: Open and Efficient Foundation Language Models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurélien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023},
	url={https://arxiv.org/abs/2302.13971}
}

@article{yang2024qwen2,
	title={Qwen2 Technical Report},
	author={Yang, Baizhou and others},
	journal={arXiv preprint arXiv:2407.10671},
	year={2024},
	url={https://arxiv.org/abs/2407.10671}
}

@misc{doan2024vintern1befficientmultimodallarge,
	title={Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese}, 
	author={Khang T. Doan and Bao G. Huynh and Dung T. Hoang and Thuc D. Pham and Nhat H. Pham and Quan T. M. Nguyen and Bang Q. Vo and Suong N. Hoang},
	year={2024},
	eprint={2408.12480},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2408.12480}, 
}

@misc{nguyen2020phobertpretrainedlanguagemodels,
	title={PhoBERT: Pre-trained language models for Vietnamese}, 
	author={Dat Quoc Nguyen and Anh Tuan Nguyen},
	year={2020},
	eprint={2003.00744},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2003.00744}, 
}

@article{zhang2023internlm,
	title={InternLM-XComposer: A Vision-Language Large Model for Advanced Image-Text Comprehension and Composition},
	author={Zhang, Yujie and others},
	journal={arXiv preprint arXiv:2309.15112},
	year={2023},
	url={https://arxiv.org/abs/2309.15112}
}

@inproceedings{chen2024internvl,
	title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
	author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={24185--24198},
	year={2024}
}

@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@article{nilsback2008automated,
	title={Automated flower classification over a large number of classes},
	author={Nilsback, M-E and Zisserman, A},
	journal={Indian Conference on Computer Vision, Graphics and Image Processing},
	year={2008},
	organization={IEEE},
	url={https://www.robots.ox.ac.uk/~vgg/data/flowers/102/}
}

@inproceedings{farhadi2010every,
	title={Every picture tells a story: Generating sentences from images},
	author={Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
	booktitle={European conference on computer vision},
	pages={15--29},
	year={2010},
	organization={Springer},
	url={https://www.cs.brown.edu/~gen/courses/cs1430/projects/sentences/}
}

@inproceedings{ordonez2011im2text,
	title={Im2text: Describing images using 1 million captioned photographs},
	author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara L},
	booktitle={Advances in neural information processing systems},
	pages={1143--1151},
	year={2011},
	url={https://www.cs.unc.edu/~vso/publication/im2text.html}
}

@inproceedings{rashtchian2010collecting,
	title={Collecting image annotations using Amazon’s Mechanical Turk},
	author={Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
	booktitle={Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk},
	pages={139--147},
	year={2010},
	url={https://hockenmaier.cs.illinois.edu/pubs/rashtchian-et-al10.pdf}
}

@article{lin2014microsoft,
	title={Microsoft coco: Common objects in context},
	author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	journal={European conference on computer vision},
	pages={740--755},
	year={2014},
	organization={Springer},
	url={https://cocodataset.org}
}

@inproceedings{vedantam2015cider,
	title={CIDEr: Consensus-based image description evaluation},
	author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4566--4575},
	year={2015}
}

@inproceedings{young2014image,
	title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
	author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
	booktitle={Transactions of the Association for Computational Linguistics},
	volume={2},
	pages={67--78},
	year={2014},
	url={https://www.aclweb.org/anthology/Q14-1006/}
}

@inproceedings{antol2015vqa,
	title={VQA: Visual question answering},
	author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={2425--2433},
	year={2015},
	url={https://visualqa.org}
}

@article{warde2015empirical,
	title={An empirical study of parameters in human evaluation of natural language generation systems},
	author={Warde-Farley, David and Bengio, Yoshua and others},
	journal={arXiv preprint arXiv:1506.06798},
	year={2015}
}

@inproceedings{krishna2017visual,
	title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
	author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David Ayman El and Bernstein, Michael S and Fei-Fei, Li},
	booktitle={International journal of computer vision},
	volume={123},
	number={1},
	pages={32--73},
	year={2017},
	url={https://visualgenome.org}
}

@article{goyal2017making,
	title={Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering},
	author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={6904--6913},
	year={2017},
	url={https://visualqa.org}
}

@inproceedings{agrawal2019nocaps,
	title={nocaps: novel object captioning at scale},
	author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chechik, Gal and Berg, Alexander and Parikh, Devi and Batra, Dhruv},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={8948--8957},
	year={2019}
}

@inproceedings{zellers2019recognition,
	title={From recognition to cognition: Visual commonsense reasoning},
	author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={6720--6731},
	year={2019}
}

@misc{sharma2018conceptual,
	title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
	author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
	year={2018},
	eprint={1803.09123},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1803.09123}
}

@misc{schuhmann2021laion400m,
	title={LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
	author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Rombach, Robin and Jenicek, Petr and Wilke, Patrick and Schulz, Carsten and others},
	year={2021},
	eprint={2111.02114},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2111.02114}
}

@article{changpinyo2021cc12m,
	title={Conceptual 12M: Pushing Web-Scale Image-Text Pretraining to Recognize Long-Tail Visual Concepts},
	author={Changpinyo, Supasorn and Sharma, Piyush and Ding, Nan and Soricut, Radu},
	journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	year={2021},
	pages={3558--3568},
	url={https://github.com/google-research-datasets/conceptual-12m}
}

@article{kazemi2017show,
	title={Show, ask, attend, and answer: A strong baseline for visual question answering},
	author={Kazemi, Vahid and Elqursh, Ali},
	journal={arXiv preprint arXiv:1611.08321},
	year={2017},
	url={https://arxiv.org/abs/1611.08321}
}

@misc{plummer2016flickr30kentitiescollectingregiontophrase,
	title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}, 
	author={Bryan A. Plummer and Liwei Wang and Chris M. Cervantes and Juan C. Caicedo and Julia Hockenmaier and Svetlana Lazebnik},
	year={2016},
	eprint={1505.04870},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1505.04870}, 
}

@misc{chen2015microsoftcococaptionsdata,
	title={Microsoft COCO Captions: Data Collection and Evaluation Server}, 
	author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
	year={2015},
	eprint={1504.00325},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1504.00325}, 
}

@misc{zhang2019showattendtranslateunpaired,
	title={Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention}, 
	author={Honglun Zhang and Wenqing Chen and Jidong Tian and Yongkun Wang and Yaohui Jin},
	year={2019},
	eprint={1811.07483},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1811.07483}, 
}

@article{schuhmann2021laion400m,
	title={LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
	author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
	journal={arXiv preprint arXiv:2111.02114},
	year={2021},
	url={https://arxiv.org/abs/2111.02114}
}

@article{changpinyo2021conceptual12m,
	title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
	author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Goodman, Sebastian},
	journal={arXiv preprint arXiv:2102.08981},
	year={2021},
	url={https://arxiv.org/abs/2102.08981}
}

@article{wang2022diffusiondb,
	title={DiffusionDB: A Large-Scale Prompt Gallery Dataset for Text-to-Image Generative Models},
	author={Wang, Zijie J. and Montoya, Evan and Munechika, David and Yang, Haoyang and Hoover, Benjamin and Chau, Duen Horng},
	journal={arXiv preprint arXiv:2210.14896},
	year={2022},
	url={https://arxiv.org/abs/2210.14896}
}

@article{bitton2022winoground,
	title={Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
	author={Bitton, Jonathan and Diwan, Anuj and Goyal, Nikhil and Kembhavi, Aniruddha and Farhadi, Ali and Parikh, Devi and Batra, Dhruv},
	journal={arXiv preprint arXiv:2204.03162},
	year={2022},
	url={https://arxiv.org/abs/2204.03162}
}

@article{saharia2022photorealistic,
	title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
	author={Saharia, Chitwan and Chang, William and Ho, Jonathan and Salimans, Tim and Ho, Jonathan and Salimans, Tim and Ho, Jonathan and Salimans, Tim and Ho, Jonathan and Salimans, Tim},
	journal={arXiv preprint arXiv:2205.11487},
	year={2022},
	url={https://arxiv.org/abs/2205.11487}
}

@article{huang2023t2icompbench,
	title={T2I-CompBench: A Comprehensive Benchmark for Open-World Compositional Text-to-Image Generation},
	author={Huang, Kaiyi and Sun, Kaiyue and Xie, Enze and Li, Zhenguo and Liu, Xihui},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	pages={78723--78747},
	year={2023},
	url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/f8ad010cdd9143dbb0e9308c093aff24-Abstract-Datasets_and_Benchmarks.html}
}

@inproceedings{liang2023richhf,
	title={Rich Human Feedback for Text-to-Image Generation},
	author={Liang, Youwei and He, Junfeng and Li, Gang and Li, Peizhao and Klimovskiy, Arseniy and Carolan, Nicholas and Sun, Jiao and Pont-Tuset, Jordi and Young, Sarah and Yang, Feng and Ke, Junjie and Dvijotham, Krishnamurthy and Collins, Katie and Luo, Yiwen and Li, Yang and Kohlhoff, Kai J and Ramachandran, Deepak and Navalpakkam, Vidhya},
	booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	year={2023},
	url={https://arxiv.org/abs/2312.10240}
}

@article{chen2023textdiffuser,
	title={TextDiffuser: Diffusion Models as Text Painters},
	author={Chen, Jingye and Huang, Yupan and Lv, Tengchao and Wei, Furu},
	journal={arXiv preprint arXiv:2305.10855},
	year={2023},
	url={https://arxiv.org/abs/2305.10855}
}

@misc{feng2024cc500,
	title     = {Concept Conjunction 500 (CC-500)},
	author    = {Feng, Weixi and He, Xuehai and Fu, Tsu-Jui and Jampani, Varun and Akula, Arjun Reddy and Narayana, Pradyumna and Basu, Sugato and Wang, Xin Eric and Wang, William Yang},
	year      = {2024},
	howpublished = {\url{https://conceptconjunction.github.io/}},
	note      = {Dataset published by the authors},
}

@misc{zhang2024abc6k,
	title        = {ABC-6K Dataset},
	author       = {Zhang, Yasi and Yu, Peiyu and Wu, Ying Nian},
	year         = {2024},
	doi          = {10.57702/nc9rsac4},
	howpublished = {\url{https://doi.org/10.57702/nc9rsac4}},
	note         = {Dataset released by the authors},
}

@misc{cho2023dallevalprobingreasoningskills,
	title={DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models}, 
	author={Jaemin Cho and Abhay Zala and Mohit Bansal},
	year={2023},
	eprint={2202.04053},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2202.04053}, 
}

@misc{schramowski2023safelatentdiffusionmitigating,
	title={Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models}, 
	author={Patrick Schramowski and Manuel Brack and Björn Deiseroth and Kristian Kersting},
	year={2023},
	eprint={2211.05105},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2211.05105}, 
}

@article{pham2024ktvic,
	title={KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain},
	author={Pham, Anh-Cuong and Nguyen, Van-Quang and Vuong, Thi-Hong and Ha, Quang-Thuy},
	journal={arXiv preprint arXiv:2401.08100},
	year={2024},
	url={https://arxiv.org/abs/2401.08100}
}

@article{lam2020uitviic,
	title={UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning},
	author={Lam, Quan Hoang and Le, Quang Duy and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy},
	journal={arXiv preprint arXiv:2002.00175},
	year={2020},
	url={https://arxiv.org/abs/2002.00175}
}

@article{bui2023uitopenviic,
	title={UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese},
	author={Bui, Doanh C. and Nguyen, Nghia Hieu and Nguyen, Khang},
	journal={arXiv preprint arXiv:2305.04166},
	year={2023},
	url={https://arxiv.org/abs/2305.04166}
}

@article{pham2024viocrvqa,
	title={ViOCRVQA: Novel Benchmark Dataset and Vision Reader for Visual Question Answering by Understanding Vietnamese Text in Images},
	author={Pham, Huy Quang and Nguyen, Thang Kien-Bao and Nguyen, Quan Van and Tran, Dan Quang and Nguyen, Nghia Hieu and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy},
	journal={arXiv preprint arXiv:2404.18397},
	year={2024},
	url={https://arxiv.org/abs/2404.18397}
}

@misc{nexdata2024vietnameseocr,
	author       = {Nexdata-AI},
	title        = {4,995 Vietnamese OCR Images Dataset},
	year         = {2024},
	url          = {https://www.nexdata.ai/datasets/ocr/1059?source=Github},
	note         = {Accessed: 2025-05-15},
	howpublished = {\url{https://www.nexdata.ai/datasets/ocr/1059?source=Github}},
	publisher    = {Nexdata-AI},
	address      = {Nexdata-AI, China},
	type         = {Dataset},
	keywords     = {Vietnamese OCR, Image Annotation, Text Recognition, Dataset}
}

@software{dinhanhx_VisualRoBERTa_2022,
	title        = {{VisualRoBERTa}},
	author       = {dinhanhx},
	year         = 2022,
	month        = 9,
	url          = {https://github.com/dinhanhx/VisualRoBERTa}
}

@article{li2023blip2,
	title={BLIP-2: Bootstrapped Language-Image Pre-Training with Frozen Image Encoders and Large Language Models},
	author={Li, Junnan and Yang, Dongxu and Su, Pengchuan and Lu, Chunyuan and Li, Xin and Le, Quoc V},
	journal={arXiv preprint arXiv:2301.12597},
	year={2023}
}

@misc{vistral2024vietnamese,
	title={Vistral-V-7B: Vietnamese Vision-Language Model},
	author={Vistral AI Team},
	year={2024},
	note={\url{https://github.com/Vistral/vistral-v-7b}}
}

@misc{paddleocr2021,
	title        = {PaddleOCR: An Open-Source OCR System Based on PaddlePaddle},
	author       = {{PaddlePaddle Authors}},
	year         = {2021},
	howpublished = {\url{https://github.com/PaddlePaddle/PaddleOCR}},
	note         = {Accessed: 2025-05-15}
}

@misc{vietocr2020,
	title        = {VietOCR: Open-Source Optical Character Recognition for Vietnamese},
	author       = {Nguyen, Hieu},
	year         = {2020},
	howpublished = {\url{https://github.com/duyquang/vietocr}},
	note         = {Accessed: 2025-05-15}
}

@misc{rombach2022highresolution,
	title        = {High-Resolution Image Synthesis with Latent Diffusion Models},
	author       = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year         = {2022},
	eprint       = {2112.10752},
	archivePrefix= {arXiv},
	primaryClass = {cs.CV},
	note         = {\url{https://arxiv.org/abs/2112.10752}}
}

@misc{stablediffusionxl2023,
	title        = {Stable Diffusion XL: Scaling Latent Diffusion Models to 1 Billion Parameters},
	author       = {Stability AI and Collaborators},
	year         = {2023},
	howpublished = {\url{https://stability.ai/blog/stable-diffusion-xl-release}},
	note         = {Accessed: 2025-05-15}
}

@misc{stablediffusion35,
	title        = {Stable Diffusion 3.5},
	author       = {Stability AI},
	year         = {2024},
	howpublished = {\url{https://stability.ai/blog/stable-diffusion-3-5-release}},
	note         = {Accessed: 2025-05-15}
}

@misc{controlnet2023,
	title        = {ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models},
	author       = {Zhang, Lvmin and Yang, Maneesh Agrawala and Efros, Alexei A.},
	year         = {2023},
	eprint       = {2302.05543},
	archivePrefix= {arXiv},
	primaryClass = {cs.CV},
	note         = {\url{https://arxiv.org/abs/2302.05543}}
}

@misc{yang2024158bitflux,
	title={1.58-bit FLUX}, 
	author={Chenglin Yang and Celong Liu and Xueqing Deng and Dongwon Kim and Xing Mei and Xiaohui Shen and Liang-Chieh Chen},
	year={2024},
	eprint={2412.18653},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2412.18653}, 
}

@misc{hamdi2025vistaocrgenerativeinteractiveend,
	title={VISTA-OCR: Towards generative and interactive end to end OCR models}, 
	author={Laziz Hamdi and Amine Tamasna and Pascal Boisson and Thierry Paquet},
	year={2025},
	eprint={2504.03621},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2504.03621}, 
}

@misc{xue2021mt5massivelymultilingualpretrained,
	title={mT5: A massively multilingual pre-trained text-to-text transformer}, 
	author={Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
	year={2021},
	eprint={2010.11934},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2010.11934}, 
}